{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Learning CY metric\n",
    "\n",
    "In this session we will implement custom loss and training functions to learn numerical approximations of Calabi Yau metrics.\n",
    "\n",
    "There has [recently](https://arxiv.org/abs/1910.08605) been interest in using ML to approximate CY metrics. The authors, however, used a combination of conventional curve fitting and tree based methods. In this notebook, we will use neural networks to approximate (the determinant of) the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "I have prepared 100000 points on the fermats quintic\n",
    "\n",
    "$$\n",
    "Q = \\sum_i z_i^5 = 0\n",
    "$$\n",
    "\n",
    "The dataset also includes some integration weights\n",
    "\n",
    "$$\n",
    "w = 5^{-2} |z_j|^{-8} (\\det(i^*\\omega^{FS}_{\\mathbb{P}^4}))^{-1}\n",
    "$$\n",
    "\n",
    "and the evaluation of the holomorphic volume form at each point\n",
    "\n",
    "$$\n",
    "\\Omega \\wedge \\bar{\\Omega}|_{p_i} = 5^{-2} |z_j|^{-8}\n",
    "$$\n",
    "\n",
    "with coordinates $p = (1,z_j, x_1, x_2, x_3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'fermat_data.npy'\n",
    "point_weights_omega = np.load(fname)\n",
    "points = np.hstack([point_weights_omega['point'].real, point_weights_omega['point'].imag]) \n",
    "weights = point_weights_omega['weight'].reshape((-1,1))\n",
    "omega = point_weights_omega['omega'].reshape((-1,1))\n",
    "features = np.concatenate((points, weights, omega), axis=-1)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(features)\n",
    "train_index = int(0.8 * len(features))\n",
    "x_train = features[0:train_index]\n",
    "y_train = x_train\n",
    "x_test = features[train_index:]\n",
    "y_test = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Custom loss\n",
    "\n",
    "We will approximate the hermitian (nfold,nfold) metric with a neural network. First, we have to write a function that takes the 9 floating point outputs and puts them into a 3x3 hermitian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hermitian(x):\n",
    "    t1 = tf.reshape(tf.complex(x, tf.zeros(9, dtype=tf.float32)), (3,3))\n",
    "    up = tf.linalg.band_part(t1, 0, -1)\n",
    "    low = tf.linalg.band_part(1j * t1, -1, 0)\n",
    "    out = up + tf.transpose(up) - tf.linalg.band_part(t1, 0, 0)\n",
    "    return out + low + tf.math.conj(tf.transpose(low))\n",
    "\n",
    "to_hermitian_batch = tf.function(lambda y_pred : tf.vectorized_map(to_hermitian, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will have to think how to optimize our neural network.\n",
    "\n",
    "There are two scalar quantities, the sigma measure\n",
    "\n",
    "$$\n",
    "\\sigma  = \\frac{1}{N_t \\text{Vol}_\\text{CY}} \\sum_{i = 1}^{N_t} | 1 - \\frac{\\omega^3(p_i) / \\text{Vol}_\\text{K} }{ \\Omega(p_i) \\wedge \\bar{\\Omega(p_i)} / \\text{Vol}_\\text{CY}} | w_i\n",
    "$$\n",
    "\n",
    "and the ricci measure\n",
    "\n",
    "$$\n",
    "|| R ||  = \\frac{1}{N_t \\text{Vol}_\\text{CY}^{2/3}} \\sum_{i = 1}^{N_t}  \\frac{\\omega^3(p_i) }{ \\Omega(p_i) \\wedge \\bar{\\Omega(p_i)} } \\lvert R(p_i) \\rvert w_i\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\t \\text{Vol}_\\text{CY} = \\frac{1}{N} \\sum_i^N w_i, \\qquad  \\text{Vol}_\\text{K} = \\frac{1}{N} \\sum_i^N \\frac{\\omega^3(p_i)}{\\Omega (p_i) \\wedge \\bar{\\Omega} (p_i) } w_i .\n",
    "$$\n",
    "\n",
    "Computing second order derivative is fairly expensive, thus we will stick to the sigma measure for this tutorial. (Note: There is an ambiguity with the absolute value in the sigma measure, which often results in ill defined metrics with neg det. We also won't check if they are in fact Ricci flat, but are only interested in the volume in this Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_loss_batch(y, g):\n",
    "\n",
    "    bSize = tf.cast(tf.shape(y)[0], dtype=tf.float32)\n",
    "    g = to_hermitian_batch(g)\n",
    "    weights, omega = y[:,-2], y[:,-1]\n",
    "    volume_cy = tf.math.reduce_sum(weights, axis=-1) / bSize\n",
    "    det = tf.math.real(tf.linalg.det(g))\n",
    "    omega_over_omega = det / omega\n",
    "    vol_k = tf.math.reduce_sum(omega_over_omega * weights, axis=-1) / bSize\n",
    "    ratio = volume_cy / vol_k\n",
    "    sigma_integrand = tf.abs(tf.ones(tf.shape(omega_over_omega), dtype=tf.float32) - omega_over_omega * ratio) * weights\n",
    "    sigma = tf.math.reduce_sum(sigma_integrand) / (bSize * volume_cy)\n",
    "    return sigma * tf.ones(tf.shape(weights), dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization\n",
    "\n",
    "Let's give this a shot. We try BO using Gaussian Processes. Also check out the very nice notebooks [here (GP)](https://krasserm.github.io/2018/03/19/gaussian-processes/) and [here (BO using GP)](https://krasserm.github.io/2018/03/21/bayesian-optimization/).\n",
    "First we have to write a function that creates our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(act, nHidden, nlayer, lr=0.001, dropout=0.1,\n",
    "               l1l2 = 0.0001, alpha = 0.99, nfold = 3):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tfk.Input(shape=(12)))\n",
    "    for i in range(nlayer):\n",
    "        if act == 0:\n",
    "            model.add(tfk.layers.Dense(nHidden, activation='tanh',\n",
    "                                       kernel_regularizer=tfk.regularizers.l1_l2(l1=l1l2, l2=l1l2)))\n",
    "        else:\n",
    "            model.add(tfk.layers.Dense(nHidden, activation='relu',\n",
    "                                       kernel_regularizer=tfk.regularizers.l1_l2(l1=l1l2, l2=l1l2)))\n",
    "        model.add(tfk.layers.Dropout(dropout))\n",
    "    model.add(tfk.layers.Dense(nfold**2))\n",
    "    model.compile(optimizer=tfk.optimizers.SGD(learning_rate=lr, momentum=alpha), \n",
    "                    loss=sigma_loss_batch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian process\n",
    "\n",
    "We use the package [BayesianOptimization](https://github.com/fmfn/BayesianOptimization), since we are lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a function to maximize. Furthermore, BO, only works with floats. Hence, we have to round the categorical and integer choices. We start with accuracy function, the quantity we want to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(act, nHidden, nlayer, lr=0.001, dropout=0.1, l1l2=0.0001,\n",
    "                 alpha = 0.99, nEpochs=10, bSize=32, verbose=0):\n",
    "    m = make_model(act, nHidden, nlayer, lr, dropout, l1l2, alpha)\n",
    "    # exception to catch ocassional tf error\n",
    "    try:\n",
    "        history = m.fit(x_train, y_train, epochs=nEpochs, batch_size=bSize,\n",
    "                   validation_data=(x_test, y_test), verbose=verbose)\n",
    "        loss = history.history['val_loss'][-1]\n",
    "    except Exception:\n",
    "        loss = 1\n",
    "        pass\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(act, nHidden, nLayer, lr, dropout, l1l2, alpha, bSize, nEpochs=10):\n",
    "    # transform to ints\n",
    "    int_act = int(round(act))\n",
    "    int_nLayer = int(round(nLayer))\n",
    "    int_nHidden = int(round(nHidden))\n",
    "    int_bSize = int(round(bSize))\n",
    "    log_lr = np.power(10,lr)\n",
    "    log_l1l2 = np.power(10,l1l2)\n",
    "    loss = get_loss(int_act, int_nHidden, int_nLayer, log_lr,\n",
    "                       dropout, log_l1l2, alpha, nEpochs, int_bSize)\n",
    "    return 1-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_optimizer = BayesianOptimization(\n",
    "    f = f,\n",
    "    pbounds={\"act\": (0,1), \"nHidden\": (32, 256), \"nLayer\": (1,4),\n",
    "             \"lr\": (-4, -1), \"dropout\": (0,0.5), \"l1l2\": (-6,-1),\n",
    "            \"alpha\": (0, 0.99), \"bSize\": (16,256)},\n",
    "    verbose=2,\n",
    "    random_state=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    act    |   alpha   |   bSize   |  dropout  |   l1l2    |    lr     |  nHidden  |  nLayer   |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.692   \u001b[0m | \u001b[0m 0.07631 \u001b[0m | \u001b[0m 0.7721  \u001b[0m | \u001b[0m 121.2   \u001b[0m | \u001b[0m 0.3617  \u001b[0m | \u001b[0m-1.11    \u001b[0m | \u001b[0m-2.385   \u001b[0m | \u001b[0m 144.3   \u001b[0m | \u001b[0m 1.216   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5232  \u001b[0m | \u001b[0m 0.2684  \u001b[0m | \u001b[0m 0.4949  \u001b[0m | \u001b[0m 179.0   \u001b[0m | \u001b[0m 0.4019  \u001b[0m | \u001b[0m-4.095   \u001b[0m | \u001b[0m-3.802   \u001b[0m | \u001b[0m 96.54   \u001b[0m | \u001b[0m 3.729   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.763   \u001b[0m | \u001b[95m 0.2134  \u001b[0m | \u001b[95m 0.4476  \u001b[0m | \u001b[95m 239.5   \u001b[0m | \u001b[95m 0.01245 \u001b[0m | \u001b[95m-2.997   \u001b[0m | \u001b[95m-1.15    \u001b[0m | \u001b[95m 83.59   \u001b[0m | \u001b[95m 2.645   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.8765  \u001b[0m | \u001b[0m 0.9091  \u001b[0m | \u001b[0m 0.1318  \u001b[0m | \u001b[0m 141.6   \u001b[0m | \u001b[0m 0.3752  \u001b[0m | \u001b[0m-2.655   \u001b[0m | \u001b[0m-2.597   \u001b[0m | \u001b[0m 77.89   \u001b[0m | \u001b[0m 2.472   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.4192  \u001b[0m | \u001b[0m 0.3724  \u001b[0m | \u001b[0m 0.4726  \u001b[0m | \u001b[0m 103.8   \u001b[0m | \u001b[0m 0.419   \u001b[0m | \u001b[0m-2.157   \u001b[0m | \u001b[0m-3.058   \u001b[0m | \u001b[0m 160.3   \u001b[0m | \u001b[0m 1.828   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.1704  \u001b[0m | \u001b[0m 0.4528  \u001b[0m | \u001b[0m 0.3494  \u001b[0m | \u001b[0m 173.8   \u001b[0m | \u001b[0m 0.1852  \u001b[0m | \u001b[0m-3.705   \u001b[0m | \u001b[0m-1.842   \u001b[0m | \u001b[0m 124.5   \u001b[0m | \u001b[0m 3.719   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.73    \u001b[0m | \u001b[0m 0.1805  \u001b[0m | \u001b[0m 0.7337  \u001b[0m | \u001b[0m 117.4   \u001b[0m | \u001b[0m 0.2132  \u001b[0m | \u001b[0m-2.828   \u001b[0m | \u001b[0m-2.431   \u001b[0m | \u001b[0m 124.9   \u001b[0m | \u001b[0m 1.004   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6823  \u001b[0m | \u001b[0m 0.09226 \u001b[0m | \u001b[0m 0.7023  \u001b[0m | \u001b[0m 141.8   \u001b[0m | \u001b[0m 0.3481  \u001b[0m | \u001b[0m-1.223   \u001b[0m | \u001b[0m-1.951   \u001b[0m | \u001b[0m 43.9    \u001b[0m | \u001b[0m 1.927   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7391  \u001b[0m | \u001b[0m 0.1301  \u001b[0m | \u001b[0m 0.7432  \u001b[0m | \u001b[0m 120.9   \u001b[0m | \u001b[0m 0.134   \u001b[0m | \u001b[0m-2.323   \u001b[0m | \u001b[0m-2.444   \u001b[0m | \u001b[0m 143.4   \u001b[0m | \u001b[0m 1.407   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.1934  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.6565  \u001b[0m | \u001b[0m 135.9   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-6.0     \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 133.3   \u001b[0m | \u001b[0m 4.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.2937  \u001b[0m | \u001b[0m 0.2455  \u001b[0m | \u001b[0m 0.4782  \u001b[0m | \u001b[0m 221.8   \u001b[0m | \u001b[0m 0.1348  \u001b[0m | \u001b[0m-3.341   \u001b[0m | \u001b[0m-1.957   \u001b[0m | \u001b[0m 87.07   \u001b[0m | \u001b[0m 2.97    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7624  \u001b[0m | \u001b[0m 0.1476  \u001b[0m | \u001b[0m 0.4631  \u001b[0m | \u001b[0m 255.7   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-2.702   \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 80.44   \u001b[0m | \u001b[0m 2.225   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6639  \u001b[0m | \u001b[0m 0.8017  \u001b[0m | \u001b[0m 0.6419  \u001b[0m | \u001b[0m 253.2   \u001b[0m | \u001b[0m 0.4277  \u001b[0m | \u001b[0m-1.863   \u001b[0m | \u001b[0m-1.173   \u001b[0m | \u001b[0m 99.05   \u001b[0m | \u001b[0m 1.894   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7145  \u001b[0m | \u001b[0m 0.08894 \u001b[0m | \u001b[0m 0.4669  \u001b[0m | \u001b[0m 244.7   \u001b[0m | \u001b[0m 0.142   \u001b[0m | \u001b[0m-4.425   \u001b[0m | \u001b[0m-1.158   \u001b[0m | \u001b[0m 62.57   \u001b[0m | \u001b[0m 3.725   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-36.4    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 160.7   \u001b[0m | \u001b[0m 0.07662 \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 32.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7416  \u001b[0m | \u001b[0m 0.05967 \u001b[0m | \u001b[0m 0.4901  \u001b[0m | \u001b[0m 122.1   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m-1.235   \u001b[0m | \u001b[0m-2.852   \u001b[0m | \u001b[0m 51.19   \u001b[0m | \u001b[0m 2.816   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.187   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 207.3   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m-6.0     \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 118.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7519  \u001b[0m | \u001b[0m 0.07467 \u001b[0m | \u001b[0m 0.89    \u001b[0m | \u001b[0m 241.4   \u001b[0m | \u001b[0m 0.3456  \u001b[0m | \u001b[0m-5.496   \u001b[0m | \u001b[0m-1.804   \u001b[0m | \u001b[0m 130.1   \u001b[0m | \u001b[0m 3.06    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-34.42   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 216.3   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 150.2   \u001b[0m | \u001b[0m 4.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5679  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 82.09   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 129.7   \u001b[0m | \u001b[0m 4.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.7496  \u001b[0m | \u001b[0m 0.3112  \u001b[0m | \u001b[0m 0.3662  \u001b[0m | \u001b[0m 94.4    \u001b[0m | \u001b[0m 0.1899  \u001b[0m | \u001b[0m-1.518   \u001b[0m | \u001b[0m-3.16    \u001b[0m | \u001b[0m 95.74   \u001b[0m | \u001b[0m 2.924   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.2969  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 61.79   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-6.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 102.3   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 73.51   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-6.0     \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 69.46   \u001b[0m | \u001b[0m 4.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7411  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 49.04   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-6.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 135.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.3974  \u001b[0m | \u001b[0m 0.8831  \u001b[0m | \u001b[0m 0.9339  \u001b[0m | \u001b[0m 64.94   \u001b[0m | \u001b[0m 0.1793  \u001b[0m | \u001b[0m-1.862   \u001b[0m | \u001b[0m-1.623   \u001b[0m | \u001b[0m 162.5   \u001b[0m | \u001b[0m 3.312   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.4732  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 31.05   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 162.6   \u001b[0m | \u001b[0m 4.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5822  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 17.33   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 131.6   \u001b[0m | \u001b[0m 4.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 28      \u001b[0m | \u001b[95m 0.7701  \u001b[0m | \u001b[95m 0.03422 \u001b[0m | \u001b[95m 0.5454  \u001b[0m | \u001b[95m 28.45   \u001b[0m | \u001b[95m 0.07031 \u001b[0m | \u001b[95m-3.917   \u001b[0m | \u001b[95m-1.54    \u001b[0m | \u001b[95m 101.8   \u001b[0m | \u001b[95m 1.031   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7654  \u001b[0m | \u001b[0m 0.2443  \u001b[0m | \u001b[0m 0.6109  \u001b[0m | \u001b[0m 41.9    \u001b[0m | \u001b[0m 0.2672  \u001b[0m | \u001b[0m-5.908   \u001b[0m | \u001b[0m-2.052   \u001b[0m | \u001b[0m 73.28   \u001b[0m | \u001b[0m 2.825   \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.1894  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 47.55   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-6.0     \u001b[0m | \u001b[0m-1.0     \u001b[0m | \u001b[0m 191.3   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "new_optimizer.maximize(\n",
    "    init_points=8,\n",
    "    n_iter=22,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A surprisingly good model\n",
    "\n",
    "Here are some hyperparameters, which I found (using BOHB) to work really well when studying this problem. In fact there are many well working combinations. In general any kind of strong regularisation tends to bring the NN to some undesired fixpoint. Some rule of thumbs, have the NN either sufficient deep or wide, ReLU seems to outperform tanh and Adam works better than standard SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlayer = 4\n",
    "nHidden = 256\n",
    "act = 1\n",
    "#lr = 0.0001\n",
    "#alpha = 0.0\n",
    "nfold = 3\n",
    "nEpochs = 30\n",
    "bSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_106 (Dense)            (None, 256)               3328      \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 9)                 2313      \n",
      "=================================================================\n",
      "Total params: 203,017\n",
      "Trainable params: 203,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tfk.Input(shape=(12)))\n",
    "for i in range(nlayer):\n",
    "    if act == 0:\n",
    "        model.add(tfk.layers.Dense(nHidden, activation='tanh'))\n",
    "    else:\n",
    "        model.add(tfk.layers.Dense(nHidden, activation='relu'))\n",
    "model.add(tfk.layers.Dense(nfold**2))\n",
    "model.compile(optimizer=tfk.optimizers.Adam(), \n",
    "                loss=sigma_loss_batch)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.7602 - val_loss: 0.5015\n",
      "Epoch 2/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.1429 - val_loss: 0.0644\n",
      "Epoch 3/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0477 - val_loss: 0.0479\n",
      "Epoch 4/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0417 - val_loss: 0.0396\n",
      "Epoch 5/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0384 - val_loss: 0.0431\n",
      "Epoch 6/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0376 - val_loss: 0.0328\n",
      "Epoch 7/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0369 - val_loss: 0.0328\n",
      "Epoch 8/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0345 - val_loss: 0.0322\n",
      "Epoch 9/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0346 - val_loss: 0.0368\n",
      "Epoch 10/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0340 - val_loss: 0.0354\n",
      "Epoch 11/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0334 - val_loss: 0.0324\n",
      "Epoch 12/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0310 - val_loss: 0.0258\n",
      "Epoch 13/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0210 - val_loss: 0.0101\n",
      "Epoch 14/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0126 - val_loss: 0.0078\n",
      "Epoch 15/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0104 - val_loss: 0.0117\n",
      "Epoch 16/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0100 - val_loss: 0.0072\n",
      "Epoch 17/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0092 - val_loss: 0.0068\n",
      "Epoch 18/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0094 - val_loss: 0.0111\n",
      "Epoch 19/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0088 - val_loss: 0.0054\n",
      "Epoch 20/30\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.0084 - val_loss: 0.0065\n",
      "Epoch 21/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0083 - val_loss: 0.0060\n",
      "Epoch 22/30\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.0080 - val_loss: 0.0051\n",
      "Epoch 23/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0076 - val_loss: 0.0116\n",
      "Epoch 24/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0084 - val_loss: 0.0078\n",
      "Epoch 25/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0086 - val_loss: 0.0056\n",
      "Epoch 26/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0077 - val_loss: 0.0134\n",
      "Epoch 27/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 28/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0072 - val_loss: 0.0066\n",
      "Epoch 29/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0075 - val_loss: 0.0046\n",
      "Epoch 30/30\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.0076 - val_loss: 0.0162\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=nEpochs, batch_size=bSize,\n",
    "                   validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature comparison\n",
    "\n",
    "Here are a couple of values from the literature using tree based methods and the classical Donaldson algorithm. The values are taken from [here](https://arxiv.org/abs/1910.08605)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "literature comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "donaldson_sigma = [0.37, 0.27, 0.19, 0.13, 0.091, 0.066, 0.051, 0.04, 0.032, 0.027, 0.023, 0.02]\n",
    "donaldson_time = [0.37, 0.8, 1.9, 9.5, 38, 170, 720, 2500, 8200, 24000, 67000, 180000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_sigma = donaldson_sigma[0:7]+[0.041, 0.034, 0.029, 0.026, 0.024, 0.021, 0.021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let us compare parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCICY import CICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fermat = CICY([[4,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of parameters learned for given k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para(Q, k):\n",
    "    deg = Q.line_co_euler([k for _ in range(Q.len)])\n",
    "    return deg**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765625.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para(Fermat, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'sigma')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13d7999908>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(donaldson_sigma)), donaldson_sigma)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('sigma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare time efficiency donaldson vs neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out from above\n",
    "average_t = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f13d783b828>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xt8VPWd//HXhxASICRcjIgQLiKCgBEh9dJ6QYstra2IXQXt1lZ3y7bWh2u1tZfdWvW3drvVqu3+rFWrXbVWu1rc4kpLvdZba4GCICKKihovEEFRrhL47B/fGTJJJpkzSU5mJvN+Ph7nMTNnzsx8Mwzznu/3nPP5mrsjIiLSnl65boCIiOQ/hYWIiGSksBARkYwUFiIikpHCQkREMlJYiIhIRgoLERHJSGEhIiIZKSxERCSj3rluQLb22WcfHz16dK6bISJSUJYuXfqOu1d39PEFFxajR49myZIluW6GiEhBMbNXO/N4DUOJiEhGCgsREclIYSEiIhkV3D4LEZGkXbt2UV9fz44dO3LdlLxRXl7OiBEjKC0t7dLnVViISMGqr69nwIABjB49GjPLdXNyzt3ZuHEj9fX1jBkzpkufO9ZhKDObaWZrzGytmX07zf1fMrMGM1ueWP4xzvaISM+yY8cOhgwZoqBIMDOGDBkSS08rtp6FmZUA1wEnAvXAYjNb4O7Ptdj0N+5+XlztEJGeTUHRXFzvR5w9i8OBte7+srt/CNwFzIrx9dJ7+mn4l38BjWmKiHRYnGExHHg95XZ9Yl1LnzOzFWZ2j5nVdHkrli2DH/wA3n23y59aRCTVpZdeylVXXdVlz7du3TomT56c9r7p06d36wnKcYZFur6Qt7h9HzDa3WuBB4Fb0z6R2TwzW2JmSxoaGrJrRVVVuNy8ObvHiYjIXnGGRT2Q2lMYAbyZuoG7b3T3nYmbNwHT0j2Ru9/o7nXuXlddnWVpk2RYvP9+do8TEYngiiuuYPz48cyYMYM1a9YAsHz5co488khqa2uZPXs27yZGNqZPn863vvUtDj/8cA466CAef/xxIPQgjjnmGKZOncrUqVN56qmnWr3O9u3bmTt3LrW1tcyZM4ft27d33x9JvIfOLgbGmdkY4A1gLnBm6gZmNszd30rcPBlY3eWtqKwMl+pZiPRoF1wAy5d37XNOmQLXXtv2/UuXLuWuu+5i2bJlNDY2MnXqVKZNm8ZZZ53Ff/7nf3LcccdxySWXcNlll3Ft4okaGxv561//ysKFC7nssst48MEH2XfffXnggQcoLy/nxRdf5Iwzzmg1xHT99dfTr18/VqxYwYoVK5g6dWrX/rEZxBYW7t5oZucBi4AS4BZ3X2VmlwNL3H0BcL6ZnQw0ApuAL3V5QzQMJSIxefzxx5k9ezb9+vUD4OSTT2br1q289957HHfccQB88Ytf5LTTTtv7mFNPPRWAadOmsW7dOiCcXHjeeeexfPlySkpKeOGFF1q91mOPPcb5558PQG1tLbW1tXH+aa3EelKeuy8EFrZYd0nK9e8A34mzDXt7FhqGEunR2usBxCnbQ1XLysoAKCkpobGxEYBrrrmGoUOH8swzz7Bnzx7Ky8u75LW6Us+vDaWehYjE5Nhjj+Xee+9l+/btfPDBB9x3333079+fQYMG7d0fcfvtt+/tZbRl8+bNDBs2jF69enH77beze/futK91xx13APDss8+yYsWKrv+D2tHzy30MGBAuFRYi0sWmTp3KnDlzmDJlCqNGjeKYY44B4NZbb+UrX/kK27Zt44ADDuCXv/xlu89z7rnn8rnPfY67776b448/nv79+7fa5qtf/Spnn302tbW1TJkyhcMPPzyWv6kt5t7yaNb8VldX51kfWzxgAHz5y3D11fE0SkRyYvXq1Rx88MG5bkbeSfe+mNlSd6/r6HP2/GEoCENR6lmIiHSYwkJERDIqjrCorNTRUCIinVAcYaGehYhIpygsREQko+IICw1DiYh0SnGEhXoWIhITM+Oiiy7ae/uqq67i0ksvBULJ8n79+rFhw4a991dUVHR3E7tEcYRFZSVs2waJU+tFRLpKWVkZ8+fP55133kl7/z777MOPf/zjbm5V1yuOsFCZchGJSe/evZk3bx7XXHNN2vvPOeccfvOb37Bp06ZublnX6vnlPqB5fajBg3PbFhGJRy5qlCd87Wtfo7a2losvvrjVfRUVFZxzzjn85Cc/4bLLLuva9nWj4uhZqPKsiMSosrKSs846i5/+9Kdp7z///PO59dZbeb+Av4OKr2chIj1TrmqUJ1xwwQVMnTqVs88+u9V9AwcO5Mwzz+RnP/tZDlrWNYqjZ6GwEJGYDR48mNNPP52bb7457f0XXnghN9xww945LApNcYSFhqFEpBtcdNFF7R4VNXv2bHbu3NnNreoaGoYSEemELVu27L0+dOhQtm3btvd28nyLpKuvvpqrC3SqhOLoWSgsREQ6pTjCoqwMSks1DCUi0kHFERZmKvkh0kMV2myfcYvr/SiOsACFhUgPVF5ezsaNGxUYCe7Oxo0bKS8v7/LnLo4d3KDKsyI90IgRI6ivr6ehoSHXTckb5eXljBgxosuft3jCQj0LkR6ntLSUMWPG5LoZRaF4hqHUsxAR6bDiCQv1LEREOkxhISIiGRVPWCSHoXTUhIhI1oonLKqqYPfuMGOeiIhkpbjCAjQUJSLSAcUTFqo8KyLSYbGGhZnNNLM1ZrbWzL7dznZ/Z2ZuZnWxNUY9CxGRDostLMysBLgO+BQwETjDzCam2W4AcD7wdFxtARQWIiKdEGfP4nBgrbu/7O4fAncBs9Js9/+AHwE7YmyLhqFERDohzrAYDryecrs+sW4vMzsMqHH3/42xHYF6FiIiHRZnWFiadXtPcjCzXsA1wEUZn8hsnpktMbMlHS4YluxZKCxERLIWZ1jUAzUpt0cAb6bcHgBMBh41s3XAkcCCdDu53f1Gd69z97rq6uqOtWbAgHCpYSgRkazFGRaLgXFmNsbM+gBzgQXJO919s7vv4+6j3X008BfgZHdfEktrSkpCYKhnISKStdjCwt0bgfOARcBq4L/dfZWZXW5mJ8f1uu1S5VkRkQ6JdT4Ld18ILGyx7pI2tp0eZ1sAFRMUEemg4jmDGxQWIiIdVFxhoWEoEZEOKa6wUM9CRKRDFBYiIpJRcYWFhqFERDqkoMNi505YsQI2bYr4gKqqMPnRrl2xtktEpKcp6LB4/XU49FC4//6ID0jWh1LvQkQkKwUdFn37hsvIM6Wq8qyISIf0iLDYvj3iA1R5VkSkQwo6LPr1C5eRw0KVZ0VEOqSgw6KsDMw60LPQMJSISFYKOizMoLxcw1AiInEr6LCAsN8i62Eo9SxERLLSI8Ii8tFQ6lmIiHRIjwiLyD2L8nLo00dhISKSpYIPi379sggLUMkPEZEOKPiwyKpnASomKCLSAQoLERHJqPjCQsNQIiJZK76wUM9CRCRrPSIsIh86CwoLEZEOKPiw0NFQIiLxK/iw6NAw1Pvvg3tsbRIR6WmKLywqK2H3bti6NbY2iYj0ND0iLD78MHz/R6LKsyIiWesRYQGwY0fEB6g+lIhI1go+LJITIGlqVRGR+BR8WGhqVRGR+CksREQko+ILCw1DiYhkrfjCQj0LEZGsxRoWZjbTzNaY2Voz+3aa+79iZivNbLmZPWFmE7N9jazDYsCAcKmwEBGJLLawMLMS4DrgU8BE4Iw0YfBrdz/E3acAPwKuzvZ1sj4aqlevEBgahhIRiSzOnsXhwFp3f9ndPwTuAmalbuDuqd/Y/YGsa3Bk3bMAFRMUEclS7xifezjwesrteuCIlhuZ2deAC4E+wAnpnsjM5gHzAEaOHNnsvg6FRWWlwkJEJAtx9iwszbpWPQd3v87dxwLfAv413RO5+43uXufuddXV1c3u63DPQsNQIiKRxRkW9UBNyu0RwJvtbH8XcEq2L6JhKBGR+MUZFouBcWY2xsz6AHOBBakbmNm4lJsnAS9m+yIahhIRiV9s+yzcvdHMzgMWASXALe6+yswuB5a4+wLgPDObAewC3gW+mO3rlJWBWQdmy9MwlIhIZHHu4MbdFwILW6y7JOX6P3f2Ncw0D7eISNwK/gxu6OAESNu3w65dsbVJRKQnidyzMLPJhJPrypPr3P22OBqVrQ71LCAMRQ0ZEkubRER6kkhhYWbfB6YTwmIh4azsJ4DCDovNmxUWIiIRRB2G+jvg48Db7n42cChQFlurstShYSjQTm4RkYiihsV2d98DNJpZJbABOCC+ZmWnX79O9CxERCSjqPsslpjZQOAmYCmwBfhrbK3KUt++HTh0FhQWIiIRRQoLdz83cfXnZvYHoNLdV8TXrOwMGADr1mXxAA1DiYhkJZujoWqB0cnHmNmB7j4/pnZlZfhweOKJLB6gnoWISFaiHg11C1ALrAL2JFY7kBdhMXIkbNoEW7dC//4RHpDsWSgsREQiidqzONLds57FrrvUJMoVvv46TJgQ4QHl5dCnj4ahREQiino01J87MuVpd0kNi8hU8kNEJLKoPYtbCYHxNrCTMFeFu3ttbC3LQnI+pNdey+JBqjwrIhJZ1LC4BfgCsJKmfRZ5Y/jwUFAw656FhqFERCKJGhavJUqK56XSUthvPw1DiYjEJWpYPG9mvwbuIwxDAZAvh85C2G+R9TDUK6/E1h4RkZ4kalj0JYTEJ1LW5c2hsxD2W6xcmcUD1LMQEYks6hncZ8fdkM6qqYGFC8E97L/ISGEhIhJZ1JPyfppm9WbC9Ki/69omdUxNTagPtWlTxKrjlZVhB3fkdBERKV5Rz7MoB6YALyaWWmAw8A9mdm1MbctK8vDZyDu5q6pgz55w2reIiLQr6j6LA4ET3L0RwMyuB/4InEg4nDbnUk/MmzIlwgNSS35UVMTWLhGRniBqz2I4kFp1qT+wv7vvJuXoqFxKhkXkI6JSp1YVEZF2Re1Z/AhYbmaPEs7ePhb4gZn1Bx6MqW1ZGTo0nG+R1TAUaCe3iEgEUY+GutnMFgKHE8Liu+7+ZuLub8bVuGz06gUjRmQRFqo8KyISWbvDUGY2IXE5FRgGvA68BuyXWJdXsjoxT8NQIiKRZepZXAjMA36css5Trp/Q5S3qhJEj4fHHI26sYSgRkcja7Vm4+7zE1euBWe5+PPAI4RyLb8TctqzV1MAbb8Du3RE21jCUiEhkUY+G+ld3f9/MjiYcLvtfhADJKzU10NgIb78dYeMBA8LJeBqGEhHJKGpYJH+rnwT8PHHWdp94mtRxWZ2Y16tXCAz1LEREMooaFm+Y2Q3A6cBCMyvL4rHdJusZ85IlP0REpF1Rv/BPBxYBM939PUKpj7w4ZDZVh07MU89CRCSjqOdZbCOlHLm7vwW8FVejOmrgwFC5I6sT8xQWIiIZxTqUZGYzzWyNma01s2+nuf9CM3vOzFaY2UNmNqpzrxd6FxqGEhHpWrGFhZmVANcBnwImAmeY2cQWmy0D6ty9FriHUFakU7I+MU89CxGRjOLsWRwOrHX3l939Q+AuYFbqBu7+SGKIC+AvwIjOvujIkVn2LBQWIiIZxRkWwwnlQZLqE+va8g/A79PdYWbzzGyJmS1paGho90VramD9etgZpRZuVZWGoUREIogzLNJNP+dp1mFmfw/UAVemu9/db3T3Onevq66ubvdFk0dE1ddHaGFVFWzfDrt2RdhYRKR4xRkW9UBNyu0RwJstNzKzGcC/ACe7e6fnxsjqxDyV/BARiSTOsFgMjDOzMWbWB5gLLEjdwMwOA24gBMWGrnjRZM/i5ZcjbKzKsyIikcQWFokpWM8jnMy3Gvhvd19lZpeb2cmJza4EKoC7zWy5mS1o4+kiGz06BMY3vwlLlmTYWJVnRUQiiTpTXoe4+0JgYYt1l6Rcn9HVr9mnDzzyCMyYASecAPffD8cc08bGGoYSEYkk7+o7dYWxY8O8FvvvD5/8JCxa1MaGGoYSEYmkR4YFhClWH3sMDjoIPvtZmD8/zUYahhIRiaTHhgXAvvuGIalp0+D00+H221tskByGUs9CRKRdPTosAAYNggcegOOOg7POgutTp2xSz0JEJJIeHxYQKtHef38Yjjr3XLgyeepfWVnYI66wEBFpV1GEBUB5Ofz2tzBnDlx8MXzve+COSn6IiEQQ66Gz+aa0FO64I/Q0/u3fQkZcW1WFqWchItKuogoLgJISuOmmMP32tdfC14dUMuq9zWkLWYmISFB0YQFhkqSrrw4HQ718eRXbF7/P2A/D7gsREWmtaPZZtGQGl10GNZOq2PXOZk49NRSgFRGR1oo2LJLGTatkzODNLFwIJ50EH3yQ6xaJiOSfog8LqqoYsOd9brstnPF94onw7ru5bpSISH5RWCQOnf37zzt33w3LlsH06WG2PRERCRQWlZWwZw9s2cLs2XDfffDii3DssVnM5S0i0sMpLFpUnv3EJ+CPf4S33w6lzV96KYdtExHJEwqLNPWhjj4aHn4YtmwJgbFqVY7aJiKSJxQWbUyANG0a/OlP4fqxx0aYdU9EpAdTWLQzAdKkSWESpQEDwqx7jz/ezW0TEckTCosMU6uOHQtPPNE0694f/9iNbRMRyRMKiwhTq7acde/ee7upbSIieUJhEXECpOSse1Onwmmnwa9+1Q1tExHJE0VZSLCZiopQKCpCmfLkrHuzZsEXvgB/+ANMmQITJ4Zl5EjopfgVkR5IYdGrV9iDHXECpOSse+eeC7//fZgfI6lfPzj44KbwSC5jxoTS6CIihUphAWEoKosJkMrL4ZZbwvVNm2D1anjuuablkUfg9tubti8rgwkTmsJj0qRwOXYs9Na/gIgUAH1VQTgiqoOz5Q0eDB/7WFhSbd4Mzz/fPET+/Ge4886mbUpLYfz41j2RceM0t4aI5BeFBcQyD3dVFRxxRFhSbdkCa9Y0BciqVbB0Kdx9d2JOcMKQ1bhxrUNk/PjQqxER6W4KCwjf7A0N3fJSFRXh7PBp05qv3769eYgkg+R3v4Pdu8M2vXrBAQe0DpEJE6B//25pvogUKYUFhGGotWtz2oS+fcORVVOmNF+/c2eogpsaIs89F3au79rVtN3o0a1D5OCDm845FBHpDIUFxDIM1VXKymDy5LCk2rUrVMRtGSIPPRQCJqmmJn2IDBrUvX+HiBQ2hQV0agd3rpSWhuGnCRPg1FOb1u/eDa+8EoawUkPk5z9vPsf4sGGtQ2TiRNhnn+7/W0Qk/8UaFmY2E/gJUAL8wt1/2OL+Y4FrgVpgrrvfE2d72lRVBTt2wIcfFvxhSCUlcOCBYZk1q2n9nj3w6quteyK//GXY6Z5UXZ0+RIYODecuikhxii0szKwEuA44EagHFpvZAnd/LmWz14AvAd+Iqx2RpNaH6qE/rXv1CicHjhkDJ53UtN4d6utbh8ivf928szVoUOsAmTQpFFhUiIj0fHH2LA4H1rr7ywBmdhcwC9gbFu6+LnHfnhjbkVlq5dkeGhZtMQv7NWpqQlXdJPcwW2DLEJk/H266qWm7ysr0PZGaGpU+EelJ4gyL4UDqLNb1wBFtbJtbESrPFhuzsF9j2DD4+Meb39fQ0HRobzJE7r+/6ax2CIfypit9Mnq0Sp+IFKI4wyLd4IR36InM5gHzAEaOHNmZNqUXsfKsBNXVcNxxYUm1cWPr0icPPQS33da0TXl589InyUWlT0TyW5z/PeuBmpTbI4A3O/JE7n4jcCNAXV1dhwKnXRkmQJJohgwJ85cffXTz9Zs3tw6RJ58M+0WS+vQJ84Wo9IlIfoozLBYD48xsDPAGMBc4M8bX6zgNQ8WqqgqOPDIsqbZsaV0/q2Xpk969W5c+OfDAcBJjaWkIkpZLaamGukS6Wmxh4e6NZnYesIhw6Owt7r7KzC4Hlrj7AjP7CHAvMAj4rJld5u6T4mpTmzQMlRMVFVBXF5ZUqaVPkvtFVq4MMxTuiXgoRK9erQOkrWCJsi7ubRVuku9iHSV294XAwhbrLkm5vpgwPJVbGobKK+2VPnnhhXDS4c6d4bSY1GXXrtbr2lqfum7nTvjgg8yPTz0zvqsp3CTfaZcihJoaZWUahspzZWVwyCFhyQX3cIZ8Z4KpI9u2XNdeuLUMwbgo3IqPwiIpywmQpPiYhX0ohXLUVqZwiyPY2gq3LVuibetdf/gKoHDrCgXyse8GBVgfSqQ9hRpu3RVi7W2bLtzSbVtI4dZZBfIx6gZ5XHlWpBikhlvfvrluTTTJnltXDTF2ZNt04ZZu285SWCSpZyEiWSopCcFWCOHW2Rpuqt6TpH0WIiJtUlgkaRhKRKRNCoskDUOJiLRJYZGU7FkoMEREWlFYJB19dNgDNGlSqLctIiJ7KSySPvlJ+MtfYOBA+Mxn4KyzYNOmXLdKRCQvKCxSfeQjoezp974Hd94ZSpzOn5/rVomI5JzCoqWyMrj8cli8OEww/bnPwZw5sGFDrlsmIpIzCou2TJkCTz8NV1wB//M/oZdx553xnd8vIpLHFBbtKS2F734X/va3MO/nmWfC7Nnw5pu5bpmISLdSWEQxaRI89RRcdRUsWgQHHBBC49e/DrWiRUR6OIVFVCUlcNFFsGIFzJsXhqg+/3moroZTTul4cGzapHM7RCTvmRfYGHxdXZ0vWbIk180I83s+9VSYMPqee8LQVFkZzJwJp50Gn/1sOCvcHdavh5degrVrw5J6/d13Q/3gOXPg3HPhiCM6X/FLRKQFM1vq7nWZt2zj8QqLLtBWcBx4IKxbB1u3Nm3bqxeMHh32gRx4YFhefhluuy30TA47LITGGWdA//65+otEpIdRWOSbPXvgz38OwfHSS2H/RjIUDjwQRo1KPxPJli1wxx1w3XWwcmUoP/KlL8FXvwrjx3fv37B1a2jDM8+EYbedO0MvqaoqXLa17LtvCEkRyTsKi57GHZ58En72s9BL2bULPv7x0NuYOTP0TJLbpT4mVa9e0eZmdIfXXguhkLq89FLTc1ZWQkVFqJu1ZUvm9u+3XwjEUaNg5Mim68nbAwdGfy9EpMsoLHqy9evh5pvh5z+H11/P/vHJuRmT8yymzrdYWhqGy1J3ro8dC4ce2nwZNappH8ru3SEw3n+/+bJ5c1jeegtefTUEUPJy587mbaqshLo6mDEDTjwxDLvl44TDIj2MwqIYNDbC738Pzz7bfOd3W9dTJzJOnV+x5VyL1dUhEGpr4ZBDYMCArm33nj3Q0NA8QF55BZ54IvRgAAYNghNOCOExY0YILO3gF+lyCgspTOvXw8MPw4MPwgMPNPWcRo1qCo6TT4Z+/XLbTpEeQmEhhc89HEb8wAMhPB5+OAxrjRkD118fKgKLSKd0Nix0Up7knhmMGxd24s+fD++8E86U79Mn7NQ/88zQExGRnFFYSP7p3Rs+8YmwX+P734ff/hYOPhh+8YuwH0REup3CQvJXWRlcemkIjUMOgS9/GaZPh9Wrc90ykWh27IAbb4RVq3Ldkk5TWEj+mzABHnkk9CyefTYcwXXppa0Py5Xis3t3mOHy0UfD8GW+ePdd+MEPwgEb//RPcNdduW5Rp2kHtxSW9evhwgtD4cbx4+GGG+C44zr+fNu2wZo1obfy/POhPMvo0eFw4tracChvXOeBuMPbb4cAfOutMIfKpEmFd97J7t2wbFk4MKGhIex/OuigsAwb1vWHQu/YEQ6E+N3vYMGC5hOTDR0a3sPJk8MyaVJYqqq6tg1tef11uOaa0JvYujXsc7v44tAjzvEh4ToaSorTokWhFMorr8DgweHM8aFD01/ut1/Y5rXXQigkg2H16nDuR/L/QK9eYXbEt94KX4AQDt2dPLkpPJLLoEHZtfe998JQxMqVIRySy8aNzberrISjjoKPfQw++tFQWLKiovPvV1dyD3/Lww+H5dFHm07uLCtr3uOrqGgKjuQyfnwI5MGDmyoSZLJpE9x/fwiIP/whfBFXVsKnPw2zZoXnWrWq6X1dtap5TbaamvDvOHZs+Dfef38YPrzpsrKyc1/mK1fClVc2TZA2dy5885uhF5wn8joszGwm8BOgBPiFu/+wxf1lwG3ANGAjMMfd17X3nAoL2WvbtvAL7sUXQ4/j7bfDsn59+6VJ+vYNX1gHH9y0TJgQfhGXlYVfrs89F+piJetjPfNM8y/2YcNCocfS0rD07t38Mnm9sTGEUn1902MHDGj65XvIIeFy6NAw//uTT4Zl1arwpVNSEr5wPvrRECCHHhpOqNy6NfyNW7c2Lam3d+5sakvyjP10S1kZlJeH9yR1abluw4amcHj44aZf82PHhpMqTzgBjj8+nOhZXw8vvBB6bC+80HR93brmpWlKSsL2Q4eGumLpLl98MQTEn/4UAnz//UM4nHJK+LWers4ahAMhXn21eYA8+2xoQ7opAfr1ax4e++4L++zTtAwZ0vx6aWn4Wx57DH70I1i4MDzHl78MX/96GH7KM3kbFmZWArwAnAjUA4uBM9z9uZRtzgVq3f0rZjYXmO3uc9p7XoWFRLJ1a1OArF8fxrNHjAihMGpU9F+0Se6hx7FiRViefz6Eyq5dYWlsbH29sTH8Wj3ooKZQmDw51MjK9Cv2vffCWPyTT4aKxk8/3fyXcnvKysKS2qausv/+TeFwwgnZfSnu2BEqLK9ZE3p5GzaEf5uWl9u3N3/cxIkhHGbNCqVisv23a2nr1vBv+cYboeRNusuGhvZ/cFRVhR8Lb74ZAu/880NPd8iQzrUtRvkcFkcBl7r7JxO3vwPg7v+ess2ixDZ/NrPewNtAtbfTKIWFFKXGxtC7ef758Eu/f/+wVFQ0v96vX+hRpHJvHhypy86d4Ut8+/awpF5PvV1REX7JH3RQvGPv7uHLPBkc1dWhWnMu7NgRepMbN4YfG6nLxo1haOyjHw3Vofv2zU0bs9DZsOideZMOGw6kVr+rB45oaxt3bzSzzcAQII8OaxDJA717w7RpYcmWWdOwU74zC8FUURHK++dSeXkYkho+PLftyBNxHjqb7udHyx5DlG0ws3lmtsTMljQ0NHRJ40REJLo4w6IeqEm5PQJ4s61tEsNQVcCmlk/k7je6e52711VXV8fUXBERaUucYbEYGGdmY8ysDzAXWNBimwXAFxPX/w54uL39FSIikhux7bNI7IM4D1hEOHT2FncxgGv+AAAGdUlEQVRfZWaXA0vcfQFwM3C7ma0l9CjmxtUeERHpuDh3cOPuC4GFLdZdknJ9B3BanG0QEZHOU20oERHJSGEhIiIZKSxERCSjgiskaGYNwKuJm/ugE/iS9F4Eeh8CvQ9N9F4E4919QEcfHOsO7ji4+94TLcxsSWdOX+9J9F4Eeh8CvQ9N9F4EZtapOkkahhIRkYwUFiIiklGhh8WNuW5AHtF7Eeh9CPQ+NNF7EXTqfSi4HdwiItL9Cr1nISIi3aBgw8LMZprZGjNba2bfznV7uouZ1ZjZI2a22sxWmdk/J9YPNrMHzOzFxGWWk0QXJjMrMbNlZva/idtjzOzpxPvwm0QRyx7PzAaa2T1m9nzis3FUMX4mzOzrif8Xz5rZnWZWXiyfCTO7xcw2mNmzKevSfgYs+Gni+3OFmU3N9PwFGRaJKVuvAz4FTATOMLOJuW1Vt2kELnL3g4Ejga8l/vZvAw+5+zjgocTtYvDPwOqU2/8BXJN4H94F/iEnrep+PwH+4O4TgEMJ70lRfSbMbDhwPlDn7pMJBUznUjyfif8CZrZY19Zn4FPAuMQyD7g+05MXZFgAhwNr3f1ld/8QuAuYleM2dQt3f8vd/5a4/gHhS2E44e+/NbHZrcApuWlh9zGzEcBJwC8Stw04AbgnsUmxvA+VwLGEKs64+4fu/h5F+JkgnDvWNzE/Tj/gLYrkM+Huj9F6PqC2PgOzgNs8+Asw0MyGtff8hRoW6aZsLbq5D81sNHAY8DQw1N3fghAowL65a1m3uRa4GNiTuD0EeM/dGxO3i+VzcQDQAPwyMST3CzPrT5F9Jtz9DeAq4DVCSGwGllKcn4mktj4DWX+HFmpYRJqOtSczswrgt8AF7v5+rtvT3czsM8AGd1+aujrNpsXwuegNTAWud/fDgK308CGndBLj8bOAMcD+QH/CcEtLxfCZyCTr/yuFGhZRpmztscyslBAUd7j7/MTq9cluZOJyQ67a100+BpxsZusIw5AnEHoaAxNDEFA8n4t6oN7dn07cvocQHsX2mZgBvOLuDe6+C5gPfJTi/EwktfUZyPo7tFDDIsqUrT1SYlz+ZmC1u1+dclfqFLVfBH7X3W3rTu7+HXcf4e6jCf/+D7v754FHCFP0QhG8DwDu/jbwupmNT6z6OPAcRfaZIAw/HWlm/RL/T5LvQ9F9JlK09RlYAJyVOCrqSGBzcriqLQV7Up6ZfZrwSzI5ZesVOW5StzCzo4HHgZU0jdV/l7Df4r+BkYT/NKe5e8udXT2SmU0HvuHunzGzAwg9jcHAMuDv3X1nLtvXHcxsCmFHfx/gZeBswo/BovpMmNllwBzCUYPLgH8kjMX3+M+Emd0JTCdU2V0PfB/4H9J8BhJh+v8JR09tA85293YLDRZsWIiISPcp1GEoERHpRgoLERHJSGEhIiIZKSxERCQjhYWIiGSksBDJITObnqyYK5LPFBYiIpKRwkKKmpn1N7P7zeyZxBwIc8xsmpn9ycyWmtmilHIJB5rZg4lt/2ZmYxNnwF6ZeOxKM5uT2Ha6mT2aMsfEHYkToZJzsTxvZk8Ap6a05TgzW55YlpnZgJy8KSJp9M68iUiPNhN4091PAjCzKuD3wCx3b0h8+V8BnAPcAfzQ3e81s3LCj61TgSmEOST2ARab2WOJ5z4MmESoufMk8DEzWwLcRKhltRb4TUpbvgF8zd2fTBSK3BHj3y2SFfUspNitBGaY2X+Y2TGE4mqTgQfMbDnwr8CIxK/84e5+L4C773D3bcDRwJ3uvtvd1wN/Aj6SeO6/unu9u+8BlgOjgQmEYncveiif8KuUtjwJXG1m5wMDU8pqi+ScehZS1Nz9BTObBnwa+HfgAWCVux+Vul1igqF00pV6TkqtP7Sbpv9vaWvsuPsPzez+RFv+YmYz3P35CH+GSOzUs5CiZmb7A9vc/VeEiXOOAKrN7KjE/aVmNikxZ0i9mZ2SWF9mZv2Ax4A5FuYCrybMWPfXdl7yeWCMmY1N3D4jpS1j3X2lu/8HsITQCxHJC+pZSLE7BLjSzPYAu4CvEiqW/jSx/6I3obrxKuALwA1mdnli29OAe4GjgGcIPYaL3f1tM0v7Re/uO8xsHnC/mb0DPEEY9gK4wMyOJ/RCniPsOxHJC6o6KyIiGWkYSkREMlJYiIhIRgoLERHJSGEhIiIZKSxERCQjhYWIiGSksBARkYwUFiIiktH/AdtcBSxgO0QFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13d78f5e48>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(donaldson_time, donaldson_sigma, color='blue', label='donald')\n",
    "plt.plot(np.arange(3, average_t*(nEpochs+1), average_t), history.history['val_loss'], color='red', label='NN')\n",
    "plt.xlabel('seconds')\n",
    "plt.xlim([-1, average_t*nEpochs + 10])\n",
    "plt.ylabel('sigma')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "ML:\n",
    "1. Try more hyperparameters.\n",
    "2. Implement your own grid search.\n",
    "3. Implement your own random search.\n",
    "\n",
    "BO:\n",
    "1. Use [TPE](https://github.com/hyperopt/hyperopt) or [Hyperband](https://arxiv.org/abs/1603.06560) (some [implementation](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html?highlight=hyperband#hyperband-tune-schedulers-hyperbandscheduler)) or [BOHB](https://github.com/automl/HpBandSter) to further tune hyperparameters.\n",
    "2. Don't work too much on this. I'm hoping to get some results published at some point :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
